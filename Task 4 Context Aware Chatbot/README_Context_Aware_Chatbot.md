
# ğŸ§  Context-Aware AI Chatbot (LangChain + LlamaCpp + Streamlit)

This project is a **private document-aware chatbot** that can answer questions based on your local `.txt` files. It uses:

- ğŸ”— LangChain for retrieval-based QA
- ğŸ§¬ HuggingFace `all-MiniLM-L6-v2` embeddings
- ğŸ¦™ LlamaCpp (TinyLLaMA) for local LLM inference
- ğŸ›ï¸ FAISS for vector storage
- ğŸ’» Streamlit for interactive UI

---
## ğŸš€ How It Works

1. Loads local `.txt` files using `DirectoryLoader`
2. Splits text into manageable chunks with `RecursiveCharacterTextSplitter`
3. Converts chunks to embeddings using HuggingFace (`all-MiniLM-L6-v2`)
4. Stores embeddings in a FAISS vector index
5. Loads `TinyLLaMA` using `LlamaCpp` and builds a `ConversationalRetrievalChain`
6. Uses full chat history as context in each new query
7. Streamlit UI allows interactive question-answering

---

## ğŸ§  Features

- âœ… Context-aware conversation (remembers past queries)
- âœ… Works **offline** using local LLM (TinyLLaMA)
- âœ… Only uses your own `.txt` files
- âœ… Interactive UI with chat history display
- âœ… Auto builds vector store if not found

---
## ğŸ§ª Screenshot

![Chatbot Screenshot](a4b199fb-efcf-4107-8e17-1954eb37c225.png)

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ dataset for chatbot/          
â”œâ”€â”€ model llm/                   
â”‚   â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
â”œâ”€â”€ faiss_index/                 
â”œâ”€â”€ chatbot_backend.py          
â”œâ”€â”€ streamlit_app.py            
â”œâ”€â”€ Task4_Context_Aware_Chatbot_Using_LangChain_or_RAG.py  
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
Recommended packages:

langchain
langchain-community
langchain-huggingface
streamlit
faiss-cpu
transformers
llama-cpp-python

Optional (for better file detection):

bash
pip install python-magic-bin


```bash
streamlit run streamlit_app.py
```

---
ğŸ“Œ Example Usage
Ask:
What are the techniques of AI?
Chatbot (based on local text files):

AI techniques include machine learning, deep learning, expert systems, and evolutionary algorithms...
ğŸ” Notes
All processing is done locally â€” no data leaves your machine.

Works best with well-structured .txt files in dataset for chatbot/

LLaMA model should be in model llm/ as a .gguf file.

## ğŸ“Œ Features

- âœ… Offline Q&A from your local documents
- âœ… Memory of full chat context
- âœ… Fast, local inference using GGUF + LlamaCpp
- âœ… Clean Streamlit interface

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ by Haris Mughal
